===========
Problem
===========
Image there is a website generates user access log in Terabytes scale every day. Each log contains many different information such as username, visiting time, location, page visited, and etc.

what's your solution if we want to only store certain info(for example, user, login time, location) to a sql database. How to optimize the process.

=================================
Considerations
=================================
-  Asynchronous : user should not be locked because of the log processing.
-  Lossless (as much as possible)
-  Latency : Are these logs being used for real-time analysis and alerting for site-down? If so < 1 min latency. Is this is used for offline reporting? 20 min latency might be doable.
-  Security : Are you sending PII data-logs for the user. If so, they must be encrypted during transfer and every service must have the right keys to de-crypt before processing

=================================
Questions to ask:
=================================
Q: How am I getting the logs? are they stored in a file or do I have rest endpoint where the log is reported to?

Q: SQL databases are not designed for heavy writing. SQL database was designed for heavy relation data reading. No SQL would be a better solution at the first point. Can we change it?

Q: Can we run an agent on client's machine?

Q: How big of a RAM do we have? can we use RAM to store data between receiving and DB insert?

=================================
Solution 1
=================================
Divide the process in two services:

-  Log-receiver: 
This service can receive logs and perform basic validations - size, data-types, required fields like user-id etc. It then batches these logs in-memory (say: 30k or 3 min) and stores this batch into a store say an S3 bucket file. (Depending on how fault-tolerant you need to be you can store each log too, since in-memory batches can be lost if the server crashes before a persist). This initial store ensures that you persist a raw copy of the log-data. (This is similar to the Producer service referred in Kafka answers)

This service can also be preceded by a queue like SQS, to decouple the log-sending service/client from this log-receiving service. Scale-handling.

-  Log-Processing worker: 
This can reads S3 file contents, transforms into the data-model needed for SQL and writes required rows into SQL. You can also drop this transformed data into another queue so you can decouple and scale writes-to-SQL independent of log transformation. SQL query engine will take care of updating indexes and atomic/consistent writes.


Partitioning 
-------------
You can either have timestamp be the partition key (which likely means you'll have hot-partitions for recent logs but good range querying) or you can have user-id be partition key(less likelihood of hot partitions for an amazon user). Or have a hash of either ts /userid be partition key (good distribution poor range querying).


=================================
Solution 2
=================================
These big amount of logs must come from several machines, so to have one agent on each machine, simply filter & extract login messages which only have user, time and location, and send these much smaller amount of messages to db for insertion. If db node still cannot handle, shard these messages and use several db machines to do the insertion.


=================================
Solution 3
=================================
Presto:
Write the data into simple csv, which is much faster and can be understood by Presto. 
Rollover the csv and let Presto write the data to distributed Hadoop SQL DB. 

Lets say each record of access log will 1KB
1024102410241024B/day = 2^40B/(2460*60s), so there will be 12400 db record/s to database. 
It a big problem for database. 
So we should store each record into csv or other file format that Presto can query.

